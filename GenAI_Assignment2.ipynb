{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "NzvSHT2tMhkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a8188e-b32d-4b8a-b309-73c0c718ddb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "key = userdata.get('AIzaSyBnRPGveJWF34fVgtB4Ryy3s8I-Z49eczg')\n",
        "genai.configure(api_key=key)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "cr-KHVaUYG6r",
        "outputId": "66ea8cc3-fe31-4ddf-c659-3f8fe854bcc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret AIzaSyBnRPGveJWF34fVgtB4Ryy3s8I-Z49eczg does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4dfd817f165d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AIzaSyBnRPGveJWF34fVgtB4Ryy3s8I-Z49eczg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.5-flash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret AIzaSyBnRPGveJWF34fVgtB4Ryy3s8I-Z49eczg does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Set your API key securely\n",
        "api_key = \"AIzaSyBnRPGveJWF34fVgtB4Ryy3s8I-Z49eczg\"\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Initialize the generative model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "print(\"Model configured successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eou5NR9-YG9F",
        "outputId": "2fd99eb5-17f5-443d-8ab3-19a068d63b7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interview_prompt = \"\"\"\n",
        "You are an AI interviewer analyzing Prompt Engineering techniques. Answer the following structured questions:\n",
        "\n",
        "Q1: What is Prompt Engineering, and why is it important?\n",
        "Q2: Explain the Interview Approach in Prompt Engineering.\n",
        "Q3: Explain Chain-of-Thought (CoT) prompting and its benefits.\n",
        "Q4: Explain Tree-of-Thought (ToT) prompting and how it differs from CoT.\n",
        "Q5: What are Zero-shot and Few-shot prompting? Compare their applications.\n",
        "Q6: Which technique works best for complex problem-solving, and why?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2THHsHTVYG_4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tot_prompt = \"\"\"\n",
        "You are an AI that solves problems using a Tree-of-Thought approach.\n",
        "\n",
        "Problem: A company wants to optimize its customer service chatbot.\n",
        "There are three potential solutions:\n",
        "1. Improve NLP understanding.\n",
        "2. Increase response speed.\n",
        "3. Enhance personalization.\n",
        "\n",
        "Break down the pros and cons of each solution in a tree-like structure, then suggest the best choice based on reasoning.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "95H_U8k-YHC0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "few_shot_prompt = \"\"\"\n",
        "Translate the following English sentences to French:\n",
        "\n",
        "Example 1:\n",
        "English: \"Hello, how are you?\"\n",
        "French: \"Bonjour, comment ça va?\"\n",
        "\n",
        "Example 2:\n",
        "English: \"Where is the nearest train station?\"\n",
        "French: \"Où est la gare la plus proche?\"\n",
        "\n",
        "Now, translate the following:\n",
        "English: \"The sky is blue and the sun is shining.\"\n",
        "French:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iIFOrXKrYHF2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run all prompts through Gemini\n",
        "responses = {\n",
        "    \"Interview Approach\": model.generate_content(interview_prompt).text,\n",
        "    \"Tree-of-Thought (ToT)\": model.generate_content(tot_prompt).text,\n",
        "    \"Few-Shot Prompting\": model.generate_content(few_shot_prompt).text,\n",
        "}\n",
        "\n",
        "# Print the results\n",
        "for key, response in responses.items():\n",
        "    print(f\"\\n📝 {key}:\\n{response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zd9sjCxqZDvh",
        "outputId": "ed819f5e-f9a2-4cfe-88d0-5fab9e7ccf3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Interview Approach:\n",
            "**Q1: What is Prompt Engineering, and why is it important?**\n",
            "\n",
            "Prompt engineering is the art and science of designing effective prompts to elicit desired outputs from large language models (LLMs).  It involves crafting input text that guides the LLM to generate relevant, accurate, and coherent responses.  It's important because the quality of the LLM's output is heavily dependent on the quality of the prompt.  A poorly designed prompt can lead to nonsensical, irrelevant, or biased results, while a well-crafted prompt can unlock the LLM's full potential, enabling creative text generation, accurate code completion, insightful analysis, and more.  Effectively, it bridges the gap between human intent and machine understanding.\n",
            "\n",
            "\n",
            "**Q2: Explain the Interview Approach in Prompt Engineering.**\n",
            "\n",
            "The \"interview\" approach in prompt engineering mimics a human interview process.  Instead of providing a single, comprehensive prompt, the user engages in a multi-turn dialogue with the LLM.  Each prompt acts as a question or a follow-up, refining the LLM's response iteratively.  This approach is particularly useful for complex tasks where a single prompt may be insufficient to convey all necessary information or to guide the LLM through the reasoning process.  The interviewer carefully structures subsequent prompts based on the LLM's previous answers, leading it towards the desired solution.\n",
            "\n",
            "\n",
            "**Q3: Explain Chain-of-Thought (CoT) prompting and its benefits.**\n",
            "\n",
            "Chain-of-Thought (CoT) prompting encourages the LLM to generate intermediate reasoning steps before arriving at a final answer.  Instead of simply asking for the answer, the prompt explicitly requests the model to show its work.  For example, instead of asking \"What is 23 multiplied by 17?\", a CoT prompt might say, \"What is 23 multiplied by 17?  Show your work step-by-step.\"  This allows the LLM to decompose complex problems into smaller, more manageable sub-problems, making it easier to identify and correct errors. The benefits include improved accuracy, especially for complex reasoning tasks, better explainability (understanding how the LLM arrived at its answer), and the ability to identify weak points in the model's reasoning.\n",
            "\n",
            "\n",
            "**Q4: Explain Tree-of-Thought (ToT) prompting and how it differs from CoT.**\n",
            "\n",
            "Tree-of-Thought (ToT) prompting extends the Chain-of-Thought approach by exploring multiple reasoning paths simultaneously.  Instead of a linear sequence of steps, ToT allows the LLM to consider different approaches, evaluate their potential, and select the most promising path.  This is similar to a decision tree where different branches represent different reasoning paths.  The key difference from CoT is that ToT considers multiple parallel lines of reasoning, allowing for more comprehensive exploration and potentially leading to better solutions for complex problems requiring exploration of various possibilities and hypotheses.  ToT generally requires more computational resources than CoT.\n",
            "\n",
            "\n",
            "**Q5: What are Zero-shot and Few-shot prompting? Compare their applications.**\n",
            "\n",
            "* **Zero-shot prompting:**  This involves providing the LLM with only the task description, without any examples. The LLM is expected to perform the task based on its pre-training.  This is suitable for tasks where the LLM has already been trained on sufficient data, and the task is relatively straightforward.\n",
            "\n",
            "* **Few-shot prompting:** This provides the LLM with a small number of examples (typically 1-10) demonstrating the desired input-output relationship. These examples guide the LLM in understanding the task and generating appropriate outputs.  This approach is generally more effective than zero-shot for tasks requiring specific formatting, nuanced understanding, or complex reasoning.\n",
            "\n",
            "\n",
            "**Q6: Which technique works best for complex problem-solving, and why?**\n",
            "\n",
            "For complex problem-solving, **Tree-of-Thought (ToT)** generally works best.  While CoT improves accuracy over simple prompting by explicitly requiring a step-by-step reasoning process, ToT significantly enhances it further by allowing for parallel exploration of different reasoning pathways.  Complex problems often involve multiple potential solutions or strategies, and ToT's ability to explore these possibilities systematically leads to a higher chance of finding the optimal or a more accurate solution.  However, the computational cost associated with ToT needs to be considered.  If computational resources are highly constrained, a well-designed CoT prompt might still be the more practical option.\n",
            "\n",
            "\n",
            "\n",
            "📝 Tree-of-Thought (ToT):\n",
            "## Optimizing Customer Service Chatbot: A Tree-of-Thought Approach\n",
            "\n",
            "**Goal:** Optimize customer service chatbot performance.\n",
            "\n",
            "**Potential Solutions:**\n",
            "\n",
            "1. **Improve NLP Understanding:**\n",
            "\n",
            "   * **Pros:**\n",
            "      * **Increased Accuracy:**  (+ Reduces misinterpretations, leading to fewer frustrated customers)\n",
            "         * (+) Higher customer satisfaction scores\n",
            "         * (+) Reduced need for human intervention\n",
            "      * **Broader Range of Queries:** (+ Handles more complex and nuanced requests)\n",
            "         * (+) Increased efficiency\n",
            "         * (+) Better problem resolution\n",
            "   * **Cons:**\n",
            "      * **High Development Cost:** (-) Requires significant investment in data, models, and expertise.\n",
            "         * (-) Long implementation time\n",
            "      * **Ongoing Maintenance:** (-)  Requires continuous training and updates to adapt to evolving language.\n",
            "         * (-) Potential for unforeseen biases in training data\n",
            "\n",
            "\n",
            "2. **Increase Response Speed:**\n",
            "\n",
            "   * **Pros:**\n",
            "      * **Improved User Experience:** (+ Faster responses lead to greater user satisfaction)\n",
            "         * (+) Reduced wait times\n",
            "         * (+) Increased perceived value\n",
            "      * **Increased Efficiency:** (+  Handles more interactions in a given time)\n",
            "         * (+) Potential for cost savings (less human agents needed)\n",
            "   * **Cons:**\n",
            "      * **Potential for Reduced Accuracy:** (-)  Prioritizing speed might compromise the quality of responses.\n",
            "         * (-) Could lead to generic or unhelpful answers\n",
            "      * **Technical Limitations:** (-)  May require significant infrastructure upgrades or optimizations.\n",
            "         * (-) Difficult to achieve significant improvements with complex queries\n",
            "\n",
            "\n",
            "3. **Enhance Personalization:**\n",
            "\n",
            "   * **Pros:**\n",
            "      * **Improved Customer Engagement:** (+ Personalized interactions foster stronger relationships)\n",
            "         * (+) Increased customer loyalty\n",
            "         * (+) Higher conversion rates (if applicable)\n",
            "      * **Targeted Assistance:** (+ Delivers more relevant information and solutions)\n",
            "         * (+) Improved problem resolution\n",
            "   * **Cons:**\n",
            "      * **Data Privacy Concerns:** (-) Requires careful handling of customer data to comply with regulations.\n",
            "         * (-) Risk of data breaches\n",
            "      * **Implementation Complexity:** (-)  Requires robust user profiling and dynamic response generation.\n",
            "         * (-) Can be computationally expensive\n",
            "\n",
            "\n",
            "**Reasoning and Choice:**\n",
            "\n",
            "Based on the tree-of-thought analysis, the best choice depends on the company's priorities and resources.  However, a prioritized approach is recommended:\n",
            "\n",
            "1. **Prioritize NLP Improvement:**  While expensive and time-consuming upfront, improving NLP understanding yields the most significant long-term benefits.  Reduced human intervention and improved accuracy outweigh the costs.\n",
            "\n",
            "2. **Secondarily, Increase Response Speed:**  Once NLP is sufficiently improved, focusing on response speed provides a significant boost to user experience and efficiency.  This step should be carefully managed to avoid compromising accuracy.\n",
            "\n",
            "3. **Personalization as a Later Enhancement:**  Personalization offers valuable benefits, but it's best implemented after addressing the foundational aspects of NLP and speed.  This approach mitigates the risk of building a personalized system that is inaccurate or slow.\n",
            "\n",
            "**Conclusion:**  A phased approach starting with improved NLP understanding, followed by increased speed, and finally personalization, offers the most robust and sustainable path to optimizing the customer service chatbot.  This minimizes risk while maximizing the return on investment.\n",
            "\n",
            "\n",
            "\n",
            "📝 Few-Shot Prompting:\n",
            "Le ciel est bleu et le soleil brille.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0l9H6eWZDyY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}